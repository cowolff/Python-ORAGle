{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retriaval\n",
    "Download of the pdf file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://bugs.python.org/file47781/Tutorial_EDIT.pdf\"\n",
    "\n",
    "def download_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    with open('temp.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    return 'temp.pdf'\n",
    "\n",
    "def pdf_to_text(filename):\n",
    "    doc = fitz.open(filename)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove double spaces\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def split_text_into_chunks(text, num_chunks):\n",
    "    words = text.split()\n",
    "    chunk_size = len(words) // num_chunks\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "pdf_file = download_pdf(pdf_url)\n",
    "pdf_text = pdf_to_text(pdf_file)\n",
    "clean_pdf_text = clean_text(pdf_text)\n",
    "\n",
    "# Decide on the number of chunks you want\n",
    "num_chunks = 5\n",
    "chunks = split_text_into_chunks(clean_pdf_text, num_chunks)\n",
    "\n",
    "# This will print the chunks, you can process them further as needed\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk[:100]}...\")  # Printing the first 100 characters of each chunk for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Using the 7B instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Function\n",
    "for ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb as db  #This helps us work with the vectors database\n",
    "from chromadb.utils import embedding_functions # This helps us fetch our embedding model\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self):\n",
    "        model, tokenizer = load_model(\"bert-base-german-cased\", \"weights/bert_german.npz\")\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        # embed the documents somehow\n",
    "        encodings = self.tokenizer(input, return_tensors=\"np\", padding=True, truncation=True)\n",
    "        input_ids = mx.array(encodings[\"input_ids\"])\n",
    "        attention_mask = mx.array(encodings[\"attention_mask\"])\n",
    "        token_type_ids = mx.array(encodings[\"token_type_ids\"])\n",
    "        sequence_output, pooled_output = model(\n",
    "            input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        return pooled_output.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Loading the data into ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def __split_string_in_half(self, s):\n",
    "    # Split the string into words\n",
    "    words = s.split()\n",
    "    \n",
    "    # Find the midpoint of the list of words\n",
    "    mid_index = len(words) // 2\n",
    "    \n",
    "    # Adjust the midpoint for even division; you can modify this as needed\n",
    "    if len(words) % 2 != 0:  # Check if the number of words is odd\n",
    "        mid_index += 1  # Adjust so the first half gets the extra word\n",
    "    \n",
    "    # Split the list into two halves\n",
    "    first_half_words = words[:mid_index]\n",
    "    second_half_words = words[mid_index:]\n",
    "    \n",
    "    # Rejoin each half into a string\n",
    "    first_half = ' '.join(first_half_words)\n",
    "    second_half = ' '.join(second_half_words)\n",
    "    \n",
    "    return first_half, second_half\n",
    "\n",
    "collection_exists = \"Green-AI\" in [collection.name for collection in self.client.list_collections()]\n",
    "if not collection_exists:\n",
    "    con = sqlite3.connect(\"crawler/mitteilungen.db\")\n",
    "    cur = con.cursor()\n",
    "\n",
    "    mitteilungen = cur.execute(\"SELECT * FROM mitteilungen\").fetchall()\n",
    "\n",
    "    con = sqlite3.connect(\"crawler/green-ai-projekte-eng.db\")\n",
    "    cur = con.cursor()\n",
    "    projekte = cur.execute(\"SELECT * FROM projekte\").fetchall()\n",
    "\n",
    "    collection = self.client.create_collection(\n",
    "        name=f\"Green-AI\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}, # l2 is the default\n",
    "        embedding_function=self.embedding_model,\n",
    "    )\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    ids = []\n",
    "    # for mitteilung in mitteilungen:\n",
    "    #     documents.append(mitteilung[2])\n",
    "    #     metadata.append({\"title\": mitteilung[1]})\n",
    "    #     ids.append(\"mitteilung \" + str(mitteilung[0]))\n",
    "    for projekt in projekte:\n",
    "        document_text = projekt[2].replace(\"\\t\", \" \")\n",
    "        document_text = document_text.replace(\"\\n\", \" \")\n",
    "        document_text = document_text.replace(\"  \", \" \")\n",
    "        for i, current_text in enumerate(self.__split_string_in_half(document_text)):\n",
    "            documents.append(current_text)\n",
    "            metadata.append({\"title\": projekt[1]})\n",
    "            ids.append(f\"projekt_{str(projekt[0])}_{i}\")\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadata,\n",
    "        ids=ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Class\n",
    "Document retrieval and prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions # This helps us fetch our embedding model\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "class RAG:\n",
    "\n",
    "    def __init__(self, translator) -> None:\n",
    "        self.embedding_model = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"intfloat/multilingual-e5-large\")\n",
    "        self.client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "        self.__init_database()\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=f\"Green-AI\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"},\n",
    "            embedding_function=self.embedding_model,\n",
    "        )\n",
    "        self.model, self.tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit-mlx\")\n",
    "        self.translator = translator\n",
    "\n",
    "    def query(self, prompt):\n",
    "        return self.collection.query(query_texts=prompt, n_results=1)\n",
    "\n",
    "    def generate_text(self, prompt: str, tasks: dict, task_id: str) -> str:\n",
    "        print(\"Started the RAG generation\")\n",
    "        translated_text = self.translator.translate_text(prompt, target_lang=\"EN-US\")\n",
    "        result = self.collection.query(\n",
    "            query_texts=translated_text.text,\n",
    "            n_results=1,\n",
    "        )\n",
    "        documents = result[\"documents\"][0][0]\n",
    "        prompt = f\"<s>[INST] You are a assistant. Answer the following question based on the attached text. Text: {documents}\\n Question: {translated_text.text}[/INST] \"\n",
    "        output = generate(self.model, self.tokenizer, prompt=prompt, verbose=False, max_tokens=1000, repetition_penalty=1.1)\n",
    "        print(output)\n",
    "        translated_output = self.translator.translate_text(output, target_lang=translated_text.detected_source_lang)\n",
    "        tasks[task_id][\"result\"] = translated_output.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Generating Answers\n",
    "Including a Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
